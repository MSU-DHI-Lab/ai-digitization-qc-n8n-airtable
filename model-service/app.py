import logging
import os
from fastapi import FastAPI, File, UploadFile, HTTPException, Request
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import List, Optional
from PIL import Image, UnidentifiedImageError
import io
import uvicorn
import time

# --- Logging Setup ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Digitization QC Model API",
    description="AI-powered quality control for collections scans.",
    version="0.2.0",
)

# --- Configuration ---
MODEL_API_TOKEN = os.getenv("MODEL_API_TOKEN")
MAX_UPLOAD_MB = float(os.getenv("MAX_UPLOAD_MB", 10))
MAX_PIXELS = int(os.getenv("MAX_PIXELS", 35_000_000))  # ~7K x 5K
ALLOWED_MIME_TYPES = {"image/jpeg", "image/png", "image/tiff"}

# --- Pydantic Models for Data Contract ---

class Defects(BaseModel):
    finger_in_frame: bool
    skew_degrees: float
    blur: str  # "none", "mild", "strong"
    glare: bool
    cutoff_edges: bool

class QualityResponse(BaseModel):
    quality: str  # "high", "low"
    score: int    # 0-100
    defects: Defects
    reasons: List[str]
    # New Airtable Features
    rich_text_report: str
    semantic_tags: List[str]
    processed_at: str

# --- Core Logic ---

def generate_rich_text_report(quality: str, score: int, defects: Defects, reasons: List[str]) -> str:
    """Generates a Markdown report for Airtable Rich Text field."""
    icon = "âœ…" if quality == "high" else "âŒ"
    report = f"## {icon} AI Analysis: {quality.upper()} Confidence ({score}/100)\n\n"
    
    if quality == "high":
        report += "**Verdict**: This scan meets digitization standards.\n"
    else:
        report += "**Verdict**: Issues detected. Please review.\n"
        
    report += "\n### Detected Issues:\n"
    has_issues = False
    if defects.finger_in_frame:
        report += "- ðŸ‘† **Finger detected** in frame\n"
        has_issues = True
    if defects.skew_degrees > 2.0:
        report += f"- ðŸ“ **Skew**: {defects.skew_degrees}Â° (Threshold: 2.0Â°)\n"
        has_issues = True
    if defects.blur != "none":
        report += f"- ðŸŒ«ï¸ **Blur**: {defects.blur.capitalize()}\n"
        has_issues = True
    if defects.cutoff_edges:
        report += "- âœ‚ï¸ **Cutoff edges** detected\n"
        has_issues = True
        
    if not has_issues:
        report += "- *None detected*\n"
        
    report += "\n---\n*Generated by AI Model Service*"
    return report

def dummy_quality_score(img: Image.Image) -> QualityResponse:
    """
    Placeholder logic.
    In a real deployment, replace this with a trained model.
    
    Example integration:
    1. Load your ONNX model:
       session = onnxruntime.InferenceSession("model.onnx")
    2. Preprocess the image (resize, normalize).
    3. Run inference:
       inputs = {session.get_inputs()[0].name: preprocessed_img}
       outputs = session.run(None, inputs)
    4. Map outputs to QualityResponse.
    """
    width, height = img.size
    pixels = width * height
    logger.info(f"Processing image: {width}x{height} pixels")

    # Default to "high quality" values
    is_high_quality = (pixels >= 2000 * 3000)
    
    if is_high_quality:
        defects = Defects(
            finger_in_frame=False,
            skew_degrees=1.5,
            blur="none",
            glare=False,
            cutoff_edges=False,
        )
        reasons = ["Large image with sufficient resolution."]
        semantic_tags = ["High Res", "Clean Scan", "Standard"]
        score = 92
        quality = "high"
    else:
        defects = Defects(
            finger_in_frame=False,
            skew_degrees=6.0,
            blur="mild",
            glare=False,
            cutoff_edges=True,
        )
        reasons = ["Image appears small or cropped."]
        semantic_tags = ["Low Res", "Needs Review", "Cropped"]
        score = 55
        quality = "low"

    rich_text = generate_rich_text_report(quality, score, defects, reasons)

    return QualityResponse(
        quality=quality,
        score=score,
        defects=defects,
        reasons=reasons,
        rich_text_report=rich_text,
        semantic_tags=semantic_tags,
        processed_at=time.strftime("%Y-%m-%dT%H:%M:%S")
    )

# --- API Endpoints ---

@app.get("/health")
async def health_check():
    """Health check endpoint for n8n or container orchestrators."""
    return {"status": "healthy", "service": "model-service"}

@app.post("/predict", response_model=QualityResponse)
async def predict(request: Request, file: UploadFile = File(...)):
    logger.info(f"Received prediction request for file: {file.filename}")

    # Optional token-based auth (simple shared secret)
    if MODEL_API_TOKEN:
        supplied = request.headers.get("x-api-token")
        if supplied != MODEL_API_TOKEN:
            logger.warning("Unauthorized request")
            raise HTTPException(status_code=401, detail="Unauthorized")

    if file.content_type not in ALLOWED_MIME_TYPES:
        logger.warning(f"Invalid content type: {file.content_type}")
        raise HTTPException(status_code=400, detail=f"File must be one of: {', '.join(sorted(ALLOWED_MIME_TYPES))}")

    contents = await file.read()
    size_mb = len(contents) / (1024 * 1024)
    if size_mb > MAX_UPLOAD_MB:
        logger.warning(f"Upload too large: {size_mb:.2f} MB")
        raise HTTPException(status_code=413, detail=f"File exceeds max size of {MAX_UPLOAD_MB} MB")

    try:
        # Verify first to catch truncated or malformed files
        with Image.open(io.BytesIO(contents)) as verify_img:
            verify_img.verify()
        img = Image.open(io.BytesIO(contents)).convert("RGB")
    except UnidentifiedImageError:
        logger.error("Could not identify image file")
        raise HTTPException(status_code=400, detail="Invalid image file")
    except Exception as exc:
        logger.error(f"Error processing image: {exc}")
        return JSONResponse(
            status_code=500,
            content={"error": f"Internal processing error: {str(exc)}"},
        )

    width, height = img.size
    if width * height > MAX_PIXELS:
        logger.warning(f"Image too large: {width}x{height}")
        raise HTTPException(status_code=413, detail=f"Image dimensions exceed allowed limit ({MAX_PIXELS} pixels)")

    result = dummy_quality_score(img)
    logger.info(f"Prediction complete. Quality: {result.quality}, Score: {result.score}")
    return result

if __name__ == "__main__":
    uvicorn.run("app:app", host="0.0.0.0", port=8000)
